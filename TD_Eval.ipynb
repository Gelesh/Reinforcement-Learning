{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal-Difference (TD) Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background:\n",
    "\n",
    "**Value function**\n",
    "It would be great to know how **good** a given state s is. Something to tell us: no matter the state we are in if we transition to state s your total reward will be x, word! If you start from s and follow policy π. That would spare us from revisiting same states over and over again. The value function does this for us. It depends on the state we are in s and the policy π your agent is following. It is given by:\n",
    "\n",
    "<img src=\"td_data/v_func.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There exists an optimal value function that has the highest value for all states. It is given by:\n",
    "\n",
    "<img src=\"td_data/v_opt.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q function**\n",
    "Agent can not control what state it ends up in, directly. It can influence enviornment by choosing some action **a**. Let us introduce another function that accepts state and action as parameters and returns the expected total reward — the Q function (it represents the **quality** of a certain action given a state). More formally, the function Q^π(s, a) gives the expected return when starting in state **s**, performing action **a** and following π.\n",
    "\n",
    "Again, we can define the optimal Q-function Q∗(s, a) that gives the expected total reward for your agent when starting at s and picks action a. That is, the optimal Q-function tells your agent how good of a choice is picking a when at state s.\n",
    "\n",
    "There is a relationship between the two optimal functions V∗ and Q∗. It is given by:\n",
    "\n",
    "<img src=\"td_data/v_and_q.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, the maximum expected total reward when starting at s is the maximum of Q∗(s, a) over all possible actions.\n",
    "\n",
    "Using Q∗(s, a) we can extract the optimal policy π∗ by choosing the action aa that gives maximum reward Q∗(s, a) for state s. We have:\n",
    "\n",
    "<img src=\"td_data/pi.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TD is a combination of ideas from Monto-Carlo methods and DP methods\n",
    "- TD methods learn directly from episodes of raw experience without a model, like MC\n",
    "- TD methods update estimates based in part on other learned estimates, without waiting for a final outcome (i.e. they bootstrap), like DP\n",
    "- TD updates a guess towards a guess\n",
    "- General Update Rule: Q[s,a] += learning_rate * (td_target - Q[s,a]); where as (td_target - Q[s,a]) is called the TD Error\n",
    "- TD Methods: \n",
    "- - Evaluation/Prediction: TD(0), TD(lambda)\n",
    "- - Control: SARSA, Q-Learning\n",
    "\n",
    "<img src=\"td_data/dp_td.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD(0) - TD Policy Evaluation \n",
    "\n",
    "Policy Evaluation focus on estimating the Vπ - for a given policy π, compute the value function Vπ.\n",
    "\n",
    "**Bellman Expectation Equation for Vπ(s):**\n",
    "<img src=\"td_data/bellman_exp.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyper-Parameters:**\n",
    "\n",
    "- **alpha:** learning rate (0 to 1) is weight given to the new information versus the old information. A factor 0 makes the agent learn nothing; while a factor of 1 makes the agent consider only the most recent information.\n",
    "\n",
    "\n",
    "- **lambda:** discount factor (0 to 1) to penalize future rewards compared to the immediate reward.\n",
    "\n",
    "<img src=\"td_data/td_eval_algo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Gridworld Enviornment\n",
    "\n",
    "<img src=\"td_data/grid.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import itertools\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Let's start by being able to run episodes in an environment.\n",
    "\n",
    "from gym.spaces import Discrete\n",
    "\n",
    "class DiscreteEnvironment:\n",
    "    \"\"\"\n",
    "    Discrete states, discrete actions. We assume rewards on actions. Formally, this means:\n",
    "    R_t = r(S_t, A_t). \"The reward is a function of the state and the action we're in\". Alternative formalizations\n",
    "    include rewards on states alone, and rewards depending on the next state as well.\n",
    "    \"\"\"\n",
    "    class TransitionMatrix:\n",
    "        def __init__(self, environment, dynamics):\n",
    "            self.env = environment\n",
    "            self.dynamics = dynamics\n",
    "            if callable(dynamics):\n",
    "                # We assume that a function takes (state, action)\n",
    "                self.transition_by_function = True\n",
    "            else:\n",
    "                self.transition_by_function = False\n",
    "                if self.dynamics.shape != (len(self.env.states), len(self.env.actions), len(self.env.states)):\n",
    "                    raise Exception(\"Transition matrix should be n_states per n_actions per n_states\")\n",
    "\n",
    "        def transition(self, state, action):\n",
    "            if self.transition_by_function:\n",
    "                return self.dynamics(state, action)\n",
    "            else:\n",
    "                return np.random.choice(self.env.states, p=self.dynamics[state, action])[0]\n",
    "\n",
    "    def __init__(self, n_states, terminal_states, n_actions, transition_matrix, rewards):\n",
    "        self.state_space = Discrete(n_states)\n",
    "        self.terminal_states = np.array(terminal_states)\n",
    "        self.nonterminal_states = [x for x in range(self.state_space.n) if x not in self.terminal_states]\n",
    "        self.action_space = Discrete(n_actions)\n",
    "        self.transition_matrix = DiscreteEnvironment.TransitionMatrix(self, transition_matrix)\n",
    "        self.rewards = np.array(rewards)\n",
    "        self.cur_state = np.random.choice(self.nonterminal_states)\n",
    "\n",
    "    def step(self, action):\n",
    "        next_state = self.transition_matrix.transition(self.cur_state, action)\n",
    "        reward = self.rewards[self.cur_state, action]\n",
    "        self.cur_state = next_state\n",
    "        self.done = self.cur_state in self.terminal_states\n",
    "        return self.cur_state, reward, self.done\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_state = np.random.choice(self.nonterminal_states)\n",
    "        self.done = False\n",
    "        return self.cur_state\n",
    "\n",
    "    # Ok, let's implement a Builder pattern\n",
    "\n",
    "    class Builder:\n",
    "        def __init__(self):\n",
    "            self._n_states = None\n",
    "            self._n_actions = None\n",
    "\n",
    "            self._terminal_states = None\n",
    "\n",
    "            self._transition_dynamics = None\n",
    "            self._rewards = None\n",
    "\n",
    "        def set_n_states(self, n_states):\n",
    "            self._n_states = n_states\n",
    "            return self\n",
    "\n",
    "        def set_terminal_states(self, list_of_terminals):\n",
    "            self._terminal_states = list(list_of_terminals)\n",
    "            return self\n",
    "\n",
    "        def set_n_actions(self, n_actions):\n",
    "            self._n_actions = n_actions\n",
    "            return self\n",
    "\n",
    "        def set_transition_dynamics(self, transitions):\n",
    "            self._transition_dynamics = transitions\n",
    "            return self\n",
    "\n",
    "        def set_rewards(self, rewards):\n",
    "            self._rewards = rewards\n",
    "            return self\n",
    "\n",
    "        def build(self):\n",
    "            return DiscreteEnvironment(\n",
    "                self._n_states,\n",
    "                self._terminal_states,\n",
    "                self._n_actions,\n",
    "                self._transition_dynamics,\n",
    "                self._rewards\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym.spaces import Discrete, Box # Let's deal with those for now.\n",
    "\n",
    "class Policy:\n",
    "    def __init__(self, state_space, action_space):\n",
    "        pass\n",
    "\n",
    "    def step(self, state):\n",
    "        pass\n",
    "\n",
    "\n",
    "class DiscretePolicy(Policy):\n",
    "    \"\"\"\n",
    "    This is the first implementation of a Policy abstraction that I've done in my life.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, state_space, action_space):\n",
    "        super(DiscretePolicy, self).__init__(state_space, action_space)\n",
    "        self.policy = np.ones((state_space.n, action_space.n)) * (1 / action_space.n)\n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "\n",
    "    def step(self, state):\n",
    "        return np.random.choice(np.arange(self.action_space.n, dtype=np.int), p=self.policy[state])\n",
    "\n",
    "\n",
    "class ContinuousStatePolicy(Policy):\n",
    "    def __init__(self, state_space, action_space):\n",
    "        super(ContinuousStatePolicy, self).__init__(state_space, action_space)\n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "\n",
    "    def step(self, state):\n",
    "        return self.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The idea is to implement TD and MonteCarlo methods to evaluate policies\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "class MonteCarlo:\n",
    "   \n",
    "    def _run_episode(env, policy, with_actions):\n",
    "        done = False\n",
    "        cur_st = env.reset()\n",
    "        rewards = [0.0]\n",
    "        visited_states = []\n",
    "        while not done:\n",
    "            action = policy.step(cur_st)\n",
    "            if with_actions:\n",
    "                visited_states.append((cur_st, action))\n",
    "            else:\n",
    "                visited_states.append(cur_st)\n",
    "            new_st, reward, done, *_ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            cur_st = new_st\n",
    "        return visited_states, rewards\n",
    "\n",
    "   \n",
    "    def state_value_eval(env, policy,\n",
    "                         discount=0.999,\n",
    "                         learning_rate=0.01,\n",
    "                         n_iter=1000,\n",
    "                         print_every=None):\n",
    "        \"\"\"\n",
    "        This is EVERY-VISIT Monte-Carlo\n",
    "        :param env: An Environment that we can reset(), step() and get observations and\n",
    "                    reward information.\n",
    "        :param policy: A strategy for behaving in an Environment. Should have a step()\n",
    "                    method that returns an action given state information.\n",
    "        :param discount: Discount factor for the MDP\n",
    "        :param learning_rate: The amount we will shift towards an error direction.\n",
    "        :param n_iter: Number of episodes to run this algorithm for\n",
    "        :param print_every: Print the current estimate of values every X iterations\n",
    "        :return: The State-Value function that shows the average return we'll have starting\n",
    "                 in each one of the states of this MDP\n",
    "        \"\"\"\n",
    "        state_values = [0.0 for _ in range(env.state_space.n)]\n",
    "\n",
    "        for episode in range(n_iter):\n",
    "            visited_states, rewards = MonteCarlo._run_episode(env, policy, with_actions=False)\n",
    "            for i, state in enumerate(visited_states):\n",
    "                if i + 1 >= len(rewards):\n",
    "                    break\n",
    "                    \n",
    "                discounted_return_from_state = np.dot(np.array(rewards[i + 1:]),\n",
    "                           np.fromfunction(lambda i: discount ** i, ((len(rewards) - i - 1),)))\n",
    "                \n",
    "                state_values[state] += learning_rate * (discounted_return_from_state - state_values[state])\n",
    "                \n",
    "            if print_every is not None and episode % print_every == 0:\n",
    "                print('State-Value estimation:\\n{}'.format(state_values))\n",
    "                \n",
    "        return state_values   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD(0) Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TDZero:\n",
    "   \n",
    "    def state_value_eval(env, policy,\n",
    "                         discount=0.999, learning_rate=0.01,\n",
    "                         n_iter=1000, print_every=None):\n",
    "        state_values = [0.0 for _ in range(env.state_space.n)]\n",
    "\n",
    "        for episode in range(n_iter):\n",
    "            done = False\n",
    "            cur_state = env.reset()\n",
    "            while not done:\n",
    "                action = policy.step(cur_state)\n",
    "                new_st, reward, done, *_ = env.step(action)\n",
    "                state_values[cur_state] += learning_rate * (reward + discount * state_values[new_st] - state_values[cur_state])\n",
    "                cur_state = new_st\n",
    "                \n",
    "            if print_every is not None and episode % print_every == 0:\n",
    "                print('State-Value estimation:\\n{}'.format(state_values))\n",
    "                \n",
    "        return state_values    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD(lambda) Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TDLambda:  \n",
    "    \n",
    "    def state_value_eval(env, policy, lamb,\n",
    "            discount=0.999,\n",
    "            learning_rate=0.01,\n",
    "            n_iter=1000,\n",
    "            print_every=None):\n",
    "        state_values = [0.0 for _ in range(env.state_space.n)]\n",
    "        for episode in range(n_iter):\n",
    "            eligibility = [0.0 for _ in range(env.state_space.n)]\n",
    "            done = False\n",
    "            cur_state = env.reset()\n",
    "            while not done:\n",
    "                for s in range(env.state_space.n):\n",
    "                    eligibility[s] *= lamb * discount\n",
    "                eligibility[cur_state] += 1\n",
    "                action = policy.step(cur_state)\n",
    "                new_st, reward, done, *_ = env.step(action)\n",
    "                error = reward + state_values[new_st] - state_values[cur_state]\n",
    "                for s in range(env.state_space.n):\n",
    "                    state_values[s] += learning_rate * eligibility[s] * error\n",
    "                    \n",
    "                cur_state = new_st\n",
    "                \n",
    "            if print_every != None and episode % print_every == 0:\n",
    "                print('Action-Value estimation:\\n{}'.format(state_values))\n",
    "        return state_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and Compare MC, TD(0) and TD(lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate using Monte Carlo!\n",
      "Monte Carlo Results:\n",
      "State values:\n",
      "{0: 0.0, 1: 0.2185337835631336, 2: 0.41634866445170965, 3: 0.61335097709002018, 4: 0.77763945743722518, 5: 0.87312916538502083, 6: 0.0}\n",
      "TD(0) Results:\n",
      "State values:\n",
      "{0: 0.0, 1: 0.10882314926460014, 2: 0.2278201004612776, 3: 0.4004911070166528, 4: 0.58361299145799783, 5: 0.7890644637231442, 6: 0.0}\n",
      "TD(lambda=0.900) Results:\n",
      "State values:\n",
      "{0: 0.0, 1: 0.15816559161946203, 2: 0.34340546392240284, 3: 0.54979230217080022, 4: 0.74041925387006846, 5: 0.87951848509006958, 6: 0.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from gym.spaces import Box, Discrete\n",
    "\n",
    "import gym\n",
    "\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "\n",
    "def simple_dynamics(state, action):\n",
    "    if action == 0:\n",
    "        new_state = state - 1\n",
    "    else:\n",
    "        new_state = state + 1\n",
    "    return new_state\n",
    "\n",
    "\n",
    "def rewards(state: int, action: int) -> int:\n",
    "    return int(simple_dynamics(state, action) == 6)\n",
    "\n",
    "\n",
    "def rewards_builder(rows, cols):\n",
    "    return np.array(\n",
    "        [rewards(*pair) for pair in zip(rows.ravel(), cols.ravel())]\n",
    "    ).reshape(rows.shape)\n",
    "\n",
    "\n",
    "env = DiscreteEnvironment.Builder()\\\n",
    "    .set_n_actions(2)\\\n",
    "    .set_n_states(7)\\\n",
    "    .set_transition_dynamics(simple_dynamics)\\\n",
    "    .set_rewards(np.fromfunction(rewards_builder, (7, 2)))\\\n",
    "    .set_terminal_states([0, 6])\\\n",
    "    .build()\n",
    "\n",
    "policy = DiscretePolicy(Discrete(7), Discrete(2))\n",
    "\n",
    "print('Evaluate using Monte Carlo!')\n",
    "# from policy_evaluation import monte_carlo_eval\n",
    "\n",
    "state_val = MonteCarlo.state_value_eval(env, policy)\n",
    "\n",
    "print('Monte Carlo Results:')\n",
    "print('State values:\\n{}'.format(\n",
    "    dict(enumerate(state_val))\n",
    "))\n",
    "\n",
    "state_val = TDZero.state_value_eval(env, policy)\n",
    "\n",
    "print('TD(0) Results:')\n",
    "print('State values:\\n{}'.format(\n",
    "    dict(enumerate(state_val))\n",
    "))\n",
    "\n",
    "lamb = 0.9\n",
    "state_val = TDLambda.state_value_eval(env, policy, lamb)\n",
    "\n",
    "print('TD(lambda={:.3f}) Results:'.format(lamb))\n",
    "print('State values:\\n{}'.format(\n",
    "    dict(enumerate(state_val))\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "289px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
